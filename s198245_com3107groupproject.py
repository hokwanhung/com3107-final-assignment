# -*- coding: utf-8 -*-
"""s198245_COM3107GroupProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aCKDV-SFNR-ekKSRdFKTnIAcTz1GwmP4

# Connect to Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import Libaray"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

"""# Read in Data"""

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Project/Credit Card Defaulter Prediction.csv')

"""# Features Describe
PAY0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months,8=payment delay for eight - months, 9=payment delay for nine months and above) 

PAY2: Repayment status in August, 2005 (scale same as above)

PAY3: Repayment status in July, 2005 (scale same as above)

PAY4: Repayment status in June, 2005 (scale same as above)

PAY5: Repayment status in May, 2005 (scale same as above)

PAY6: Repayment status in April, 2005 (scale same as above)

BILLAMT1: Amount of bill statement in September, 2005 (NT dollar) 

BILLAMT2: Amount of bill statement in August, 2005 (NT dollar)

BILLAMT3: Amount of bill statement in July, 2005 (NT dollar) 

BILLAMT4: Amount of bill statement in June, 2005 (NT dollar)

BILLAMT5: Amount of bill statement in May, 2005 (NT dollar) 

BILLAMT6: Amount of bill statement in April, 2005 (NT dollar)

PAYAMT1: Amount of previous payment in September, 2005 (NT dollar) 

PAYAMT2: Amount of previous payment in August, 2005 (NT dollar)

PAYAMT3: Amount of previous payment in July, 2005 (NT dollar) 

PAYAMT4: Amount of previous payment in June, 2005 (NT dollar)

PAYAMT5: Amount of previous payment in May, 2005 (NT dollar) 

PAYAMT6: Amount of previous payment in April, 2005 (NT dollar)

# Check Data

# the unique count of default
"""

data['default '].value_counts()

data.shape

data.head(10)

data = data.drop(columns = ["ID"], axis = 1) # ID is useless in such case

data.info() # Getting the data info

# DataFrame.isnull() returns a boolean same-sized object if the values are NA(True).
null_values = (data.isnull().sum()) #*100/len(data) would only change it to float64, useless
null_values

# Generates descriptive statistics.
data_statistics = data.describe()
data_statistics = data_statistics.rename(index={'count': 'row_counts', '50%': 'median'})
data_statistics.drop(columns=['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])

"""# Get Dummy Variables"""

dummies = pd.get_dummies(data, columns = ['SEX', 'EDUCATION', 'MARRIAGE'])

dummies.iloc[0:, 21:]

import warnings 
# ignore and never print any matching warnings, why??
warnings.filterwarnings(action= 'ignore')
# Change all defaults into 0 and 1 as well
dummies.loc[dummies['default '] == 'Y', 'default '] = 1
dummies.loc[dummies['default '] == 'N', 'default '] = 0

dummies = dummies.astype('int')

dummies.info()

"""# Best subset selection

Best subset selection is a method that aims to find the subset of independent variables (Xi) that best predict the outcome (Y) and it does so by considering all possible combinations of independent variables.

Step 1: Consider all possible models

Step 2: Identify the best model of each size

Step 3: Identify the best overall model

# Import Libaray
"""

# Commented out IPython magic to ensure Python compatibility.
import itertools
import time
import seaborn as sns
import statsmodels.api as sm
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
from tqdm import tnrange, tqdm_notebook

# %matplotlib inline
plt.style.use('ggplot') # something similar to matploblib

"""# Helper function for fitting linear regression (Sklearn)

Linear regression:

is a measurement that helps determine the strength of the relationship between a dependent variable and one or more other factors, known as independent or explanatory variables.

Residual sum of squares:

The smaller the residual sum of squares, the better your model fits your data; the greater the residual sum of squares, the poorer your model fits your data. 

Helper function:

a function that performs part of the computation of another function following the DRY (Don't repeat yourself) concept.


It takes very long time to finish the step (about 6 hours)
"""

def fit_linear_reg(X,Y):
    #Fit linear regression model and return RSS and R squared values
    model_k = linear_model.LinearRegression(fit_intercept = True)
    model_k.fit(X,Y)
    RSS = mean_squared_error(Y,model_k.predict(X)) * len(Y) # RSS = Residual Sum of Squares --> absolute amount of variance (residue) not explained by the regression model.
    R_squared = model_k.score(X,Y) # R_Squared = the absoute amount of variation as a proportion of total variation.
    return RSS, R_squared

#Importing tqdm for the progress bar
from tqdm import tnrange, tqdm_notebook

#Initialization variables
Y = dummies["default "] # default = y
X = dummies.drop(columns = ["default "], axis = 1)
k = 11 # what the hell is k???
RSS_list, R_squared_list, feature_list = [],[], []
numb_features = []

#Looping over k = 1 to k = 11 features in X
for k in tnrange(1,len(X.columns) + 1, desc = 'Loop...'): # X.culumns = 33 (after drop and dummies) + 1(end_range-1), desc=description??? (Have progress bar)

    #Looping over all possible combinations: from 11 choose k
    for combo in itertools.combinations(X.columns,k): # used to creation of iterators which helps us in efficient looping in terms of space as well as time.
        tmp_result = fit_linear_reg(X[list(combo)],Y)   #Store temp result 
        RSS_list.append(tmp_result[0])                  #Append lists (Append each RSS per column into the list)
        R_squared_list.append(tmp_result[1])            #Append lists (Append each R_squared per column into the list)
        feature_list.append(combo) # combinations list per columns
        numb_features.append(len(combo))   

#Store in DataFrame
BestModule = pd.DataFrame({'numb_features': numb_features,'RSS': RSS_list, 'R_squared':R_squared_list,'features':feature_list})

"""# Finding the best subsets for each number of features
Using the smallest RSS value(predicted outcome with less error), or the largest R_squared value (largest percentage of variance)
"""

BestModule_min = BestModule[BestModule.groupby('numb_features')['RSS'].transform(min) == BestModule['RSS']] # get the minimum of column 'RSS' for each 'numb_features' group using transform
BestModule_max = BestModule[BestModule.groupby('numb_features')['R_squared'].transform(max) == BestModule['R_squared']]
display(BestModule_min.head(3))
display(BestModule_max.head(3))

"""# Adding columns to the dataframe with RSS and R squared values of the best subset"""

BestModule['min_RSS'] = BestModule.groupby('numb_features')['RSS'].transform(min)
BestModule['max_R_squared'] = BestModule.groupby('numb_features')['R_squared'].transform(max)
BestModule.head() #returns the first 5 rows

"""# Plotting the best subset selection process"""

fig = plt.figure(figsize = (16,6))
ax = fig.add_subplot(1, 2, 1)

ax.scatter(BestModule.numb_features,BestModule.RSS, alpha = .2, color = 'darkblue' )
ax.set_xlabel('# Features')
ax.set_ylabel('RSS')
ax.set_title('RSS - Best subset selection')
ax.plot(BestModule.numb_features,BestModule.min_RSS,color = 'r', label = 'Best subset')
ax.legend()

ax = fig.add_subplot(1, 2, 2)
ax.scatter(BestModule.numb_features,BestModule.R_squared, alpha = .2, color = 'darkblue' )
ax.plot(BestModule.numb_features,BestModule.max_R_squared,color = 'r', label = 'Best subset')
ax.set_xlabel('# Features')
ax.set_ylabel('R squared')
ax.set_title('R_squared - Best subset selection')
ax.legend()

plt.show()

"""# Forward stepwise selection
Forward selection is a type of stepwise regression which begins with an empty model and adds in variables one by one.
"""

#Importing tqdm for the progress bar
from tqdm import tnrange, tqdm_notebook

#Initialization variables
Y = dummies["default "]
X = dummies.drop(columns = ["default "], axis = 1)
k = 32

remaining_features = list(X.columns.values)
features = []
RSS_list, R_squared_list = [np.inf], [np.inf] #Due to 1 indexing of the loop...
features_list = dict()

for i in range(1,k+1):
    best_RSS = np.inf
    
    for combo in itertools.combinations(remaining_features,1):

            RSS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result 

            if RSS[0] < best_RSS:
                best_RSS = RSS[0]
                best_R_squared = RSS[1] 
                best_feature = combo[0]

    #Updating variables for next loop
    features.append(best_feature)
    remaining_features.remove(best_feature)
    
    #Saving values for plotting
    RSS_list.append(best_RSS)
    R_squared_list.append(best_R_squared)
    features_list[i] = features.copy()

print('Forward stepwise subset selection')
print('Number of features |', 'Features |', 'RSS')
# display([(i,features_list[i], round(RSS_list[i])) for i in range(1,33)])
print(features_list)
print(RSS_list)

# create rounded list of RSS
rounded_RSS_list = []
for j in range(1,33):
    rounded_RSS_list.append(round(RSS_list[j]))
print(rounded_RSS_list)

rounded_RSS_min = min(rounded_RSS_list)
print(rounded_RSS_min)

min_RSS_list = []
for k in range(0, len(rounded_RSS_list)): # len(rounded_RSS_list) = 32
    if rounded_RSS_list[k] == rounded_RSS_min:
        min_RSS_list.append(k+1) # rounded_RSS_list starts at index 0 instad of index 1
print(min_RSS_list)

display([features_list[l] for l in min_RSS_list]) # min_RSS_list = [26, 27, 28, 29, 30, 31, 32]

"""# Our model should be
26 - 32 model with same RSS value(4522)

Those 6 variables are
'PAY_AMT3',
'BILL_AMT5',
'EDUCATION_University',
'SEX_M',
'MARRIAGE_Other'

We are going to use the full model

# Model set up
"""

features_dummies = dummies.drop(columns = ["default "], axis = 1)
outcomes_dummies = dummies["default "]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features_dummies, outcomes_dummies, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.2, random_state = 42 )

"""# Split the data into Training and Testing"""

features = X_train
targets = y_train
features_test = X_test
targets_test = y_test

print(features[:10])
print(targets[:10])

"""# Train the 2-layer Neural Network
The following function trains the 2-layer neural network. First, we'll write some useful functions.
"""

# Activation (sigmoid) function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def sigmoid_prime(x):
    return sigmoid(x) * (1-sigmoid(x))
def error_formula(y, output):
    return - y*np.log(output) - (1 - y) * np.log(1-output)

# The error term formula
def error_term_formula(x, y, output):
    return (y - output)*(sigmoid_prime(x))

# Neural network hyperparameters
epochs = 1000
learnrate = 0.5

# Training function
def train_nn(features, targets, epochs, learnrate):
    
    # Use to same seed to make debugging easier
    np.random.seed(42)

    n_records, n_features = features.shape
    last_loss = None

    # Initialize weights
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)

    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features.values, targets):
            # Loop through all records, x is the input, y is the target

            # Activation of the output unit
            #   Notice we multiply the inputs and the weights here 
            #   rather than storing h as a separate variable 
            output = sigmoid(np.dot(x, weights))

            # The error, the target minus the network output
            error = error_formula(y, output)

            # The error term
            #   Notice we calulate f'(h) here instead of defining a separate
            #   sigmoid_prime function. This just makes it faster because we
            #   can re-use the result of the sigmoid function stored in
            #   the output variable
            error_term = error_term_formula(x, y, output)

            # The gradient descent step, the error times the gradient times the inputs
            del_w += error_term * x

        # Update the weights here. The learning rate times the 
        # change in weights, divided by the number of records to average
        weights += learnrate * del_w / n_records

        # Printing out the mean square error on the training set
        if e % (epochs / 10) == 0:
            out = sigmoid(np.dot(features, weights))
            loss = np.mean((out - targets) ** 2)
            print("Epoch:", e)
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            print("=========")
    print("Finished training!")
    return weights
    
weights = train_nn(features, targets, epochs, learnrate)

"""# Calculate the Accuracy on the Testing Data"""

tes_out = sigmoid(np.dot(features_test, weights))
predictions = tes_out > 0.5
accuracy = np.mean(predictions == targets_test)
print("Prediction accuracy: {:.3f}".format(accuracy))

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print('The prediction accuracy on the training data is', train_accuracy)
print('The prediction accuracy on the testing data is', test_accuracy)

model = DecisionTreeClassifier(max_depth=6, min_samples_leaf=6, min_samples_split=10)
model.fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print('The training accuracy is', train_accuracy)
print('The test accuracy is', test_accuracy)

"""# LinearRegression()

"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

logistic = LogisticRegression(random_state = 42)
logistic.fit(X_train,y_train)

y_train_logistic_pred = logistic.predict(X_train)
y_test_logistic_pred = logistic.predict(X_test)

train_logistic_accuracy = accuracy_score(y_train, y_train_logistic_pred)
test_logistic_accuracy = accuracy_score(y_test, y_test_logistic_pred)

print('The training accuracy is', train_logistic_accuracy)
print('The test accuracy is', test_logistic_accuracy)

"""# k nearest neighbour"""

from sklearn.neighbors import KNeighborsClassifier

k_neighbors = KNeighborsClassifier()
k_neighbors.fit(X_train,y_train)

y_train_k_pred = k_neighbors.predict(X_train)
y_test_k_pred = k_neighbors.predict(X_test)

train_k_accuracy = accuracy_score(y_train, y_train_k_pred)
test_k_accuracy = accuracy_score(y_test, y_test_k_pred)

print('The training accuracy is', train_k_accuracy)
print('The test accuracy is', test_k_accuracy)

"""# Random forest classifier
# Random forest give the highest accuracy
"""

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier(random_state = 42)
random_forest.fit(X_train, y_train)

ye_train = random_forest.predict(X_train)
ye_test = random_forest.predict(X_test)

train_ye_accuracy = accuracy_score(y_train, ye_train)
test_ye_accuracy = accuracy_score(y_test, ye_test)

print('The training accuracy is', train_ye_accuracy)
print('The test accuracy is', test_ye_accuracy)

"""# plot the one decision tree"""

from sklearn.tree import plot_tree, export_text
plt.figure(figsize=(80,20))

random_forest.estimators_[4] 

plot_tree(random_forest.estimators_[0], feature_names = X_train.columns, max_depth = 3, filled = True);

importance_df = pd.DataFrame({'feature':X_train.columns,'Importance': random_forest.feature_importances_}).sort_values('Importance', ascending = False)

plt.figure(figsize=(10,5))

sns.barplot(data = importance_df.head(10), x = 'Importance', y = 'feature');
plt.title('Feature Importance')

"""# Testing n_estimators"""

model1 = RandomForestClassifier(random_state = 42, n_jobs = 1, n_estimators = 100)
model1.fit(X_train, y_train)
model1.score(X_train, y_train), model1.score(X_val, y_val)

model1 = RandomForestClassifier(random_state = 42, n_jobs = 1, n_estimators = 300)
model1.fit(X_train, y_train)
model1.score(X_train, y_train), model1.score(X_val, y_val)

model1 = RandomForestClassifier(random_state = 42, n_jobs = 1, n_estimators = 500)
model1.fit(X_train, y_train)
model1.score(X_train, y_train), model1.score(X_val, y_val)

"""# Testing max depth"""

def test_params(**params):
    model2 = RandomForestClassifier(random_state = 42, n_jobs = 1, **params).fit(X_train, y_train)
    return model2.score(X_train, y_train), model2.score(X_val, y_val)

test_params(max_depth = 10)

test_params(max_depth = 20)

test_params(max_depth = 30)

"""# Testing max_leaf_nodes"""

test_params(max_leaf_nodes = 2**5)

test_params(max_leaf_nodes = 2**10)

test_params(max_leaf_nodes = 2**15)

"""# Testing max_features"""

test_params(max_features = 'log2')

test_params(max_features = 5)

test_params(max_features = 15)

"""# Testing min_samples_split and min_samples_leaf"""

test_params(min_samples_split=100, min_samples_leaf=50)

test_params(min_samples_split=150, min_samples_leaf=50)

test_params(min_samples_split=100, min_samples_leaf=60)

test_params(min_samples_split=150, min_samples_leaf=60)

"""# Final Model"""

final_model = RandomForestClassifier(random_state = 42, n_jobs = 1, n_estimators = 300,
                     min_samples_split = 100, min_samples_leaf=60, max_features = 'log2',
                    max_leaf_nodes = 2**10,max_depth = 20  )
final_model.fit(X_train, y_train)
final_model.score(X_train, y_train), final_model.score(X_val, y_val)

"""# Result"""

result = RandomForestClassifier(random_state = 42, n_jobs = 1, n_estimators = 300,
                     min_samples_split = 100, min_samples_leaf=60, max_features = 'log2',
                    max_leaf_nodes = 2**10,max_depth = 20  )
result.fit(X_train, y_train)
result.score(X_train, y_train), result.score(X_test, y_test)

result_1 = RandomForestClassifier(random_state = 42, n_jobs = 1, n_estimators = 300,
                     min_samples_split = 100, min_samples_leaf=60, max_features = 'log2',
                    max_leaf_nodes = 2**10,max_depth = 20  )
result_1.fit(X_train, y_train)
yee = result_1.predict(X_train)
y_prede = result_1.predict(X_test)
accuracy_score(yee, y_train), accuracy_score(y_prede, y_test)

``